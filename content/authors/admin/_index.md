---
role: MSc Graduate
avatar_filename: https://avatars.githubusercontent.com/u/15103527?v=4
organizations:
  - name: RLAI, University of Alberta
    url: http://rlai.ualberta.ca
superuser: true
authors:
  - admin
title: Subho 
interests:
  - Reinforcement Learning
  - Representation Learning
  - Continual Learning
social:
  - icon: envelope
    icon_pack: fas
    link: mailto:spramanik@ualberta.ca
  - icon: google-scholar
    icon_pack: ai
    link: https://scholar.google.com/citations?user=qi8GdxIAAAAJ&hl=en
  - icon: github
    icon_pack: fab
    link: https://github.com/subho406
  - icon: linkedin
    icon_pack: fab
    link: https://www.linkedin.com/in/subho406/
  - icon: twitter
    icon_pack: fab
    link: https://twitter.com/subho_in
education:
  courses:
    - course: MSc in Computer Science (thesis based, Fully funded)
      institution: University of Alberta
      year: 2021 - 2023
    - course: B.Tech in Computer Science and Engineering
      institution: Vellore Institute of Technology
      year: 2015 - 2019
email: email [at] subho [dot] in
user_groups:
  - Researchers
  - Visitors
---
> **"** *I think; therefore I am.* **"** --- René Descartes.

I research AI. My research interests lie broadly in reinforcement learning, representation learning and continual learning. Over the years, I have developed significant expertise in sequential decision making algorithms, specifically transformers and RNNs. 

I completed my MSc in Computing Science at the University of Alberta and was co-supervised by [Adam White](https://sites.ualberta.ca/~amw8/) and [Marlos Machado](http://mcmachado.info); affliated with [RLAI Lab](http://rlai.ualberta.ca) and [Alberta Machine Intelligence Institute (Amii)](https://www.amii.ca). In my MSc thesis, I proposed a recurrent alternative to the transformer’s self-attention mechanism, which offers context-independent inference cost and parallelization over an input sequence. The proposed approach called the Recurrent Linear Transformer was shown to outperform state-of-the-art transformers and recurrent neural networks in partially observable reinforcement learning problems, both in terms of computational efficiency and performance. ([Thesis URL](https://drive.google.com/file/d/1Xy8X_PmKgGM8r1VpsG6FduaUbODmy1f1/view?usp=drive_link), [Arxiv preprint](https://arxiv.org/abs/2310.15719)). 

I also have several years of industry experience in AI. I’m currently a senior machine learning engineer at an early-stage computer vision startup, where I apply transformers for image segmentation in architectural diagrams. During my MSc, I interned at Huawei Research Edmonton, applying reinforcement learning to neural network operator fusion. Previously, I worked with IBM Cloud as an ML Engineer (for around 2 years) and collaborated with IBM Research on various research projects in natural language processing and multi-modal learning. I also helped deploy several machine learning algorithms at scale in IBM and Kone.

**Contact:** spramanik \[at] ualberta \[dot] ca, email \[at] subho \[dot] in
