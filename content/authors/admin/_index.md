---
role: MSc Graduate
avatar_filename: https://avatars.githubusercontent.com/u/15103527?v=4
organizations:
  - name: RLAI, University of Alberta
    url: http://rlai.ualberta.ca
superuser: true
authors:
  - admin
title: Subho (Subhojeet Pramanik)
interests:
  - Reinforcement Learning
  - Representation Learning
  - Deep Learning
social:
  - icon: envelope
    icon_pack: fas
    link: mailto:spramanik@ualberta.ca
  - icon: google-scholar
    icon_pack: ai
    link: https://scholar.google.com/citations?user=qi8GdxIAAAAJ&hl=en
  - icon: github
    icon_pack: fab
    link: https://github.com/subho406
  - icon: linkedin
    icon_pack: fab
    link: https://www.linkedin.com/in/subho406/
  - icon: twitter
    icon_pack: fab
    link: https://twitter.com/subho_in
education:
  courses:
    - course: MSc in Computer Science (thesis based, Fully funded)
      institution: University of Alberta
      year: 2021 - 2023
    - course: B.Tech in Computer Science and Engineering
      institution: Vellore Institute of Technology
      year: 2015 - 2019
email: email [at] subho [dot] in
user_groups:
  - Researchers
  - Visitors
---
> **"** *The truly unique feature of our language is not its ability to transmit information about men and lions. Rather, itâ€™s the ability to transmit information
> about things that do not exist at all. As far as we know, only Sapiens can talk about entire kinds of entities that they have never seen, touched or smelled.* **"** --- Yuval Noah Harrari

I work in reinforcement learning and artificial intelligence. I completed my MSc in Computing Science at the University of Alberta and was co-supervised by [Adam White](https://sites.ualberta.ca/~amw8/) and [Marlos Machado](http://mcmachado.info); affliated with [RLAI Lab](http://rlai.ualberta.ca) and [Alberta Machine Intelligence Institute (Amii)](https://www.amii.ca). 

My research interests lie broadly in reinforcement learning, representation learning and continual learning. In my MSc thesis I proposed a recurrent alternative to the transformer's self-attention mechanism that offers context-independent inference cost and parallelization over an input suquence. The proposed self-attention mechanism was shown to outperform state-of-the-art transformers and recurrent neural networks in partially observable reinforcement learning problems, both in terms of computational efficiency and performance. 

I have also worked in several industry positions in machine learning. During my MSc, I interned at Huawei Research Edmonton, applying reinforcement learning to neural network operator fusion. Previously, I had worked with IBM Cloud as an ML Engineer (around 2 years) and collaborated with IBM Research on various research projects in representation learning and deep learning. I also helped deploy several machine learning algorithms at scale in IBM and Kone.

**Contact:** spramanik \[at] ualberta \[dot] ca, email \[at] subho \[dot] in
