---
# Documentation: https://sourcethemes.com/academic/docs/managing-content/

title: Text normalization using memory augmented neural networks
subtitle: ''
summary: ''
authors:
- Subhojeet Pramanik
- Aman Hussain
tags:
- '"Text normalization"'
- '"Differentiable Neural Computer"'
- '"Deep learning"'
categories: []
date: '2019-05-01'
lastmod: 2020-09-11T21:02:12+05:30
featured: true
draft: false

# Featured image
# To use, add an image named `featured.jpg/png` to your page's folder.
# Focal points: Smart, Center, TopLeft, Top, TopRight, Left, Right, BottomLeft, Bottom, BottomRight.
image:
  caption: ''
  focal_point: ''
  preview_only: false

# Projects (optional).
#   Associate this post with one or more of your projects.
#   Simply enter your project's folder or file name without extension.
#   E.g. `projects = ["internal-project"]` references `content/project/deep-learning/index.md`.
#   Otherwise, set `projects = []`.
projects: []
publishDate: '2020-09-11T15:32:29.611967Z'
publication_types:
- 2
abstract: We perform text normalization, i.e. the transformation of words from the
  written to the spoken form, using a memory augmented neural network. With the addition
  of dynamic memory access and storage mechanism, we present a neural architecture
  that will serve as a language-agnostic text normalization system while avoiding
  the kind of unacceptable errors made by the LSTM-based recurrent neural networks.
  By successfully reducing the frequency of such mistakes, we show that this novel
  architecture is indeed a better alternative. Our proposed system requires significantly
  lesser amounts of data, training time and compute resources. Additionally, we perform
  data up-sampling, circumventing the data sparsity problem in some semiotic classes,
  to show that sufficient examples in any particular class can improve the performance
  of our text normalization system. Although a few occurrences of these errors still
  remain in certain semiotic classes, we demonstrate that memory augmented networks
  with meta-learning capabilities can open many doors to a superior text normalization
  system.
publication: '*Speech Communication*'
url_pdf: http://www.sciencedirect.com/science/article/pii/S0167639318302395
doi: https://doi.org/10.1016/j.specom.2019.02.003
---
